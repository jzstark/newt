{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
    "def data_type(): return tf.float32\n",
    "train_size = train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: (1) Preparing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "\n",
    "train_data = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "validation_data  = mnist.validation.images\n",
    "validation_labels = mnist.validation.labels\n",
    "\n",
    "train_data = train_data.reshape(train_data.shape[0], IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "validation_data = validation_data.reshape(validation_data.shape[0], IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: (2) Define Input placeholder and weight variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_node = tf.placeholder(\n",
    "#      data_type(),\n",
    "#      shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "#train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE, NUM_LABELS))\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "      data_type(),\n",
    "      shape=(None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.int64, shape=(None, NUM_LABELS))\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED, dtype=data_type()))\n",
    "conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n",
    "conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [5, 5, 32, 64], stddev=0.1,\n",
    "      seed=SEED, dtype=data_type()))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "      tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED,\n",
    "                          dtype=data_type()))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n",
    "                                                stddev=0.1,\n",
    "                                                seed=SEED,\n",
    "                                                dtype=data_type()))\n",
    "fc2_biases = tf.Variable(tf.constant(\n",
    "      0.1, shape=[NUM_LABELS], dtype=data_type()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (3): Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = tf.nn.conv2d(train_data_node,\n",
    "                    conv1_weights,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='SAME')\n",
    "relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "pool = tf.nn.max_pool(relu,\n",
    "                    ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1],\n",
    "                    padding='SAME')\n",
    "conv = tf.nn.conv2d(pool,\n",
    "                    conv2_weights,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='SAME')\n",
    "relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "pool = tf.nn.max_pool(relu,\n",
    "                    ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1],\n",
    "                    padding='SAME')\n",
    "pool_shape = pool.get_shape().as_list()\n",
    "reshape = tf.reshape(\n",
    "    pool,\n",
    "    #[pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]]\n",
    "    [tf.shape(pool)[0], pool_shape[1] * pool_shape[2] * pool_shape[3]]\n",
    ")\n",
    "hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "# if train:\n",
    "hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "logits = tf.matmul(hidden, fc2_weights) + fc2_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training (4): Define Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      labels=train_labels_node, logits=logits))\n",
    "#loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#      labels=train_labels_node, logits=logits))\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "loss += 5e-4 * regularizers\n",
    "batch = tf.Variable(0, dtype=data_type())\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "      0.01,                # Base learning rate.\n",
    "      batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "      train_labels.shape[0],          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(train_labels_node, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing (5): Start Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 0.00), 7.1 ms\n",
      "Minibatch loss: 8.280, learning rate: 0.010000\n",
      "Minibatch error: 0.2%\n",
      "Validation error: 0.1%\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in xrange(int(NUM_EPOCHS * train_size) // BATCH_SIZE):\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        #batch_data, batch_labels = mnist.train.next_batch(BATCH_SIZE)\n",
    "        feed_dict = {train_data_node: batch_data,\n",
    "                    train_labels_node: batch_labels}\n",
    "        # Run the optimizer to update weights.\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        # print some extra information once reach the evaluation frequency\n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            # fetch some extra nodes' data\n",
    "            l, lr, predictions, acc = sess.run([loss, learning_rate, prediction, accuracy],\n",
    "                                      feed_dict=feed_dict)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print('Step %d (epoch %.2f), %.1f ms' %\n",
    "                  (step, float(step) * BATCH_SIZE / train_size,\n",
    "                   1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "            print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "            print('Minibatch error: %.1f%%' %  acc)\n",
    "            print('Validation error: %.1f%%' % accuracy.eval(feed_dict={\n",
    "                train_data_node: validation_data,\n",
    "                train_labels_node: validation_labels\n",
    "            }))\n",
    "            \n",
    "            # error_rate(\n",
    "            #    eval_in_batches(validation_data, sess), validation_labels))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
