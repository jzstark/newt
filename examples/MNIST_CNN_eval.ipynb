{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to use the pre-trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "def data_type(): return tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "\n",
    "test_data = mnist.test.images\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1: should I reshape my data? (1, 784) or (1, 28, 28) or (None, 28, 28, 1) or (64, 28, 28, 1)? \n",
    "(None, ) or (None 10) for the layer?\n",
    "-- that requires a way to know the spec of input layer\n",
    "\n",
    "Now assume I know that the input layer is (None, 28, 28, 1) for image and (None, 10) for the labels, so...\n",
    "\"\"\"\n",
    "\n",
    "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2: Let's assume that there is a convenient \"accuracy\" function in the original \n",
    "-- and we know that, with all its description, then it's good. Let's not re-invent the wheel.\n",
    "So that requires a method to add these variables during model-saving and a method to get them out.\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load graph, including the definition of \"accuracy\".\n",
    "    # Load weight\n",
    "    \n",
    "    print('Validation error: %.1f%%' % accuracy.eval(feed_dict={\n",
    "                train_data_node: test_data,\n",
    "                train_labels_node: test_labels\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "But that may not happen -- the trainer may not define the \"accuracy\" parameter at all, \n",
    "or that the shape of \"accuracy\" tensor is not suitable here, \n",
    "which are both quite reasonable. So the inferencer have to build from loaded models.\n",
    "We assume the \"logits\" part is always necessary in a saved model, so...\n",
    "\"\"\"\n",
    "\n",
    "# We need to define suitable labels, with known parameter\n",
    "test_labels_node = tf.placeholder(\n",
    "      tf.float32, #  data_type() \n",
    "      shape=(None, 28, 28, 1))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      labels=test_labels_node, logits=logits))\n",
    "\n",
    "# ALL of these weights' name(fc_*)and \"where\" they are \n",
    "# -- which means the model should be visualized for the inferencer to use\n",
    "\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "\n",
    "loss += 5e-4 * regularizers\n",
    "batch = tf.Variable(0, dtype=tf.float32)  # data_type()) -- again, need to know the precision\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "      0.01,                # Base learning rate.\n",
    "      batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "      test_labels.shape[0],          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(test_labels_node, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# And then the normal use \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load graph, including the definition of \"accuracy\".\n",
    "    # Load weight\n",
    "    \n",
    "    print('Validation error: %.1f%%' % accuracy.eval(feed_dict={\n",
    "        # Note that even though we know the test_lables_name now, \n",
    "        # the train_data_node as input layer is still hidden behind the\n",
    "        # definition of \"logit\".\n",
    "        train_data_node: test_data, \n",
    "        test_labels_node: test_labels        \n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "But in cases I may find myself not happy with the input format, and need to change it to my own input shape.\n",
    "And in this example, we may also want to use another way to \n",
    "\"\"\"\n",
    "\n",
    "# Load graph.\n",
    "\n",
    "EVAL_BATCH_SIZE = 64 # Should we also load some constant? Maybe\n",
    "  \n",
    "# This step requires the change of the first input layer to eval_data:\n",
    "# train_data_node = tf.placeholder(\n",
    "#      data_type(),\n",
    "#      shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "#  eval_data = tf.placeholder(\n",
    "#      data_type(),\n",
    "#      shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "# They differ in input batch size\n",
    "\n",
    "eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "\n",
    "# This function is just an example of a different method the inferencer want to use \n",
    "# to measure accuracy.\n",
    "def error_rate(predictions, labels):\n",
    "    return 100.0 - (\n",
    "      100.0 *\n",
    "      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])\n",
    "\n",
    "def eval_in_batches(data, sess):\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "          raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            predictions[begin:end, :] = sess.run(\n",
    "                eval_prediction,\n",
    "                feed_dict={eval_data: data[begin:end, ...]})\n",
    "        else:\n",
    "            batch_predictions = sess.run(\n",
    "                eval_prediction,\n",
    "                feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "            predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load weight\n",
    "    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Also in many cases, we may want to drop some layers \n",
    "-- such as the dropout layer in a training model but unnecessary in a test model;\n",
    "we may also want to drop/change the output layer (and add more) layers.\n",
    "\n",
    "We call it \"Layer Manipulation\". That must be supported. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-train/Fine-tune a model, there might be two cases:\n",
    "1) I want to fine-tune the existing MNIST model for, say, notMNIST dataset,\n",
    "which means Inference might not be the only purpose here; \n",
    "2) I want to use InceptionV3 for MNIST -- leave out of the scope now... \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
